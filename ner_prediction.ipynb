{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NER Predict with BertForTokenClassification - [HAILab-PUCPR](https://github.com/HAILab-PUCPR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer,AutoTokenizer\n",
    "import numpy as np\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predictBERTNER(sentencas,MODEL_DIR):\n",
    "        \n",
    "    model = torch.load(MODEL_DIR + '/torch_model',map_location=torch.device('cpu'))\n",
    "    tokenizer = BertTokenizer.from_pretrained(MODEL_DIR, do_lower_case=True) # lower or not, this is important\n",
    "\n",
    "    with open(MODEL_DIR + '/idx2tag.json', 'r') as filehandle:\n",
    "        idx2tag = json.load(filehandle) \n",
    "        \n",
    "    predictedModel=[]\n",
    "    \n",
    "    for test_sentence in sentencas:\n",
    "        tokenized_sentence = tokenizer.encode(test_sentence)\n",
    "        input_ids = torch.tensor([tokenized_sentence])#.cuda()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            output = model(input_ids)\n",
    "        label_indices = np.argmax(output[0].to('cpu').numpy(), axis=2)\n",
    "        \n",
    "        # join bpe split tokens\n",
    "        tokens = tokenizer.convert_ids_to_tokens(input_ids.to('cpu').numpy()[0])\n",
    "        new_tokens, new_labels = [], []\n",
    "        for token, label_idx in zip(tokens, label_indices[0]):\n",
    "            if token.startswith(\"##\"):\n",
    "                new_tokens[-1] = new_tokens[-1] + token[2:]\n",
    "            else:\n",
    "                new_labels.append(label_idx)\n",
    "                new_tokens.append(token)\n",
    "            \n",
    "        FinalLabelSentence = []\n",
    "        for token, label in zip(new_tokens, new_labels):\n",
    "            label = idx2tag[str(label)]\n",
    "            #print(\"{}\\t{}\".format(label, token))   #Comment\n",
    "            if label == \"O\" or label == \"X\":\n",
    "                FinalLabelSentence.append(\"O\")\n",
    "            else:\n",
    "                FinalLabelSentence.append(label)\n",
    "                \n",
    "        predictedModel.append(FinalLabelSentence[1:-1]) # delete [SEP] and [CLS]\n",
    "        \n",
    "            \n",
    "    return predictedModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['paciente', 'com', 'sepse', 'pulmonar', 'em', 'd8', 'tazocin', '(', 'paciente', 'não', 'recebeu', 'por', '2', 'dias', 'atb', ')', '.'], ['acesso', 'venoso', 'central', 'em', 'subclavia', 'd', 'duplolumen', 'recebendo', 'solução', 'salina', 'e', 'glicosada', 'em', 'bi', '.']]\n"
     ]
    }
   ],
   "source": [
    "import nltk    \n",
    "from nltk import tokenize    \n",
    "\n",
    "# THE MODEL ACCEPTS ONLY LOWER\n",
    "test_sentence1 = \"Paciente com Sepse pulmonar em D8 tazocin (paciente não recebeu por 2 dias Atb).\".lower()\n",
    "test_sentence2 = \"Acesso venoso central em subclavia D duplolumen recebendo solução salina e glicosada em BI.\".lower()\n",
    "\n",
    "test_sentence_tokenized = [tokenize.word_tokenize(test_sentence1, language='portuguese'),tokenize.word_tokenize(test_sentence2, language='portuguese')] \n",
    "print(test_sentence_tokenized)"
   ]
  },
   {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please download the [NER model](https://github.com/HAILab-PUCPR/BioBERTpt/tree/master/model) and unzip here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['O',\n",
       "  'O',\n",
       "  'B-C',\n",
       "  'I-C',\n",
       "  'I-C',\n",
       "  'I-C',\n",
       "  'I-C',\n",
       "  'O',\n",
       "  'O',\n",
       "  'B-N',\n",
       "  'B-THER',\n",
       "  'O',\n",
       "  'I-DT',\n",
       "  'I-DT',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O'],\n",
       " ['B-AS',\n",
       "  'B-AS',\n",
       "  'O',\n",
       "  'O',\n",
       "  'B-AS',\n",
       "  'I-R',\n",
       "  'X',\n",
       "  'X',\n",
       "  'X',\n",
       "  'X',\n",
       "  'O',\n",
       "  'B-AS',\n",
       "  'O',\n",
       "  'I-R',\n",
       "  'O']]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MODEL_DIR = r\"biobert-all-clinpt\"\n",
    "tags = predictBERTNER(test_sentence_tokenized,MODEL_DIR)\n",
    "tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "paciente O\n",
      "com O\n",
      "sepse B-C\n",
      "pulmonar I-C\n",
      "em I-C\n",
      "d8 I-C\n",
      "tazocin I-C\n",
      "( O\n",
      "pciente O\n",
      "não B-N\n",
      "recebeu B-THER\n",
      "por O\n",
      "2 I-DT\n",
      "dias I-DT\n",
      "atb O\n",
      ") O\n",
      ". O\n",
      "\n",
      "acesso B-AS\n",
      "venoso B-AS\n",
      "central O\n",
      "em O\n",
      "subclavia B-AS\n",
      "d I-R\n",
      "duplolumen X\n",
      "recebendo X\n",
      "solução X\n",
      "salina X\n",
      "e O\n",
      "glicosada B-AS\n",
      "em O\n",
      "bi I-R\n",
      ". O\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for s,sa in zip(test_sentence_tokenized,tags):\n",
    "    for t, a in zip(s,sa):\n",
    "        print(t,a)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reference: \n",
    "#CH: Characterization; \n",
    "#T: Test; \n",
    "#EV: Evolution; \n",
    "#G: Genetics; \n",
    "#AS: Anatomical Site; \n",
    "#N: Negation; \n",
    "#OBS: Additional Observations; \n",
    "#C: Condition; \n",
    "#R: Results; \n",
    "#DT: DateTime; \n",
    "#THER: Therapeutics; \n",
    "#V: Value; \n",
    "#RA: Route of Administration; \n",
    "#O: Out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
